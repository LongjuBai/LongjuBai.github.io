<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://longjubai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://longjubai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-04T05:19:53+00:00</updated><id>https://longjubai.github.io/feed.xml</id><title type="html">Longju Bai’s Homepage</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Brief Summary on Positional Encoding Methods</title><link href="https://longjubai.github.io/blog/2024/Positional-Encoding/" rel="alternate" type="text/html" title="A Brief Summary on Positional Encoding Methods"/><published>2024-08-22T00:00:00+00:00</published><updated>2024-08-22T00:00:00+00:00</updated><id>https://longjubai.github.io/blog/2024/Positional-Encoding</id><content type="html" xml:base="https://longjubai.github.io/blog/2024/Positional-Encoding/"><![CDATA[<h2 id="preliminary">Preliminary</h2> <p>Unlike recurrent architectures, transformers process inputs in parallel all at once. However, self-attention doesn’t consider the position of a word in a sequence; thus, sentences like “I think therefore I am” and “I am therefore I think” are treated the same.</p> <p>The notations are shown below:</p> <ul> <li>\(S_N = \{ w_i \}_{i=1}^N\): a sequence of \(N\) input tokens with \(w_i\) being the \(i\)-th element.</li> <li>\(E_N = \{ x_i \}_{i=1}^N\): corresponding word embeddings of \(S_N\); \(x_i \in \mathbb{R}^d\) is the \(d\)-dimensional word embedding vector of token \(w_i\) without position information.</li> </ul> <p>\(q_m\), \(k_n\), and \(v_n\) incorporate the \(m\)-th and \(n\)-th positions through \(f_q\), \(f_k\), and \(f_v\), respectively.</p> \[q_m = f_q(x_m, m), \quad k_n = f_k(x_n, n), \quad v_n = f_v(x_n, n) \label{eq:1}\] <p>The query and key values are then used to compute the attention weights, while the output is computed as the weighted sum over the value representations.</p> \[a_{m,n} = \frac{\exp\left( \frac{ q_m^\top k_n }{ \sqrt{d} } \right)}{ \sum_{j=1}^{N} \exp\left( \frac{ q_m^\top k_j }{ \sqrt{d} } \right)}, \quad o_m = \sum_{n=1}^{N} a_{m,n} v_n \label{eq:2}\] <p>The existing approaches of transformer-based positional encoding mainly focus on choosing a suitable function to form Equation \eqref{eq:1}.</p> <hr/> <h2 id="absolute-positional-encoding-ape">Absolute Positional Encoding (APE)</h2> <p>There are usually two forms of APE:</p> <ul> <li> <p><strong>Trainable position vectors</strong> \(\boldsymbol{p}_{i} \in \{ \boldsymbol{p}_{t} \}_{t=1}^{L}\), where \(L\) is the maximum sequence length.</p> \[f_{t \in \{ q, k, v \} }\left( \boldsymbol{x}_{i}, i \right) := \boldsymbol{W}_{t \in \{ q, k, v \} } \left( \boldsymbol{x}_{i} + \boldsymbol{p}_{i} \right) \label{eq:3}\] </li> <li> <p><strong>Generate \(\boldsymbol{p}_{i}\) using the sinusoidal function:</strong></p> \[\begin{cases} \boldsymbol{p}_{i, 2t} = \sin \left( \frac{i}{10000^{2t / d}} \right), \\ \boldsymbol{p}_{i, 2t+1} = \cos \left( \frac{i}{10000^{2t / d}} \right) \end{cases} \label{eq:4}\] </li> </ul> <p>The first form is used in the GPT-2 model; thus, when facing unseen input lengths, there will be no position embeddings beyond the maximum length encountered during training.</p> <hr/> <h2 id="relative-positional-encoding-repe">Relative Positional Encoding (RePE)</h2> <p>We introduce the original RePE proposed by Shaw et al. in 2018 and a simplified version used in the T5 model:</p> <ul> <li> <p><strong>Shaw’s Version:</strong></p> \[\begin{aligned} f_{q}\left( \boldsymbol{x}_{m} \right) &amp;= \boldsymbol{W}_{q} \boldsymbol{x}_{m}, \\ f_{k}\left( \boldsymbol{x}_{n}, n \right) &amp;= \boldsymbol{W}_{k} \left( \boldsymbol{x}_{n} + \tilde{\boldsymbol{p}}_{r}^{k} \right), \\ f_{v}\left( \boldsymbol{x}_{n}, n \right) &amp;= \boldsymbol{W}_{v} \left( \boldsymbol{x}_{n} + \tilde{\boldsymbol{p}}_{r}^{v} \right) \end{aligned} \label{eq:5}\] <p>where \(\tilde{\boldsymbol{p}}_{r}^{k}, \tilde{\boldsymbol{p}}_{r}^{v} \in \mathbb{R}^{d}\) are trainable relative position embeddings. Note that \(r = \operatorname{clip}\left( m - n, r_{\min}, r_{\max} \right)\) represents the relative distance between positions \(m\) and \(n\).</p> <p>For example, if we set the maximum relative length to be 4, then a sequence of 5 words will have a total of 9 embeddings to be learned (1 embedding for the current word, 4 embeddings for the words to the left, and 4 embeddings for the words to the right of the current word).</p> </li> <li> <p><strong>Simplified Version Used in T5 Model:</strong></p> <p>The relative position encoding is applied directly as a scalar bias to the logits of the attention mechanism.</p> <p>Specifically, the scalar bias for each relative position \(r = m - n\) is added to the dot product of the query and key vectors before the softmax operation.</p> <p>The attention logit for token \(m\) attending to token \(n\) is computed as:</p> \[\text{Logit}(m, n) = \frac{ \boldsymbol{q}_m \cdot \boldsymbol{k}_n }{ \sqrt{d} } + b_r\] </li> </ul> <hr/> <h2 id="rotary-positional-encoding-rope">Rotary Positional Encoding (RoPE)</h2> <p>Instead of simply adding positional embedding vectors, RoPE applies a rotation to the representation vectors before computing their dot products, integrating both absolute and relative position information.</p> <h3 id="2d-form">2D Form</h3> <p>For a 2-dimensional space, the transformation can be described as:</p> \[f_{\{ q, k \} }\left( \boldsymbol{x}_{m}, m \right) = \begin{pmatrix} \cos m \theta &amp; -\sin m \theta \\ \sin m \theta &amp; \cos m \theta \end{pmatrix} \boldsymbol{W}_{ \{ q, k \} } \boldsymbol{x}_m\] <p>where:</p> <ul> <li>\(\boldsymbol{W}_{\{ q, k \} }\) are the weight matrices for queries and keys.</li> <li>\(m\) is the current position index.</li> <li>\(\theta\) is a constant that determines the rate of rotation per position step, effectively encoding the absolute position.</li> </ul> <p>The attention mechanism then calculates the dot product between these rotated embeddings.</p> <h3 id="general-form">General Form</h3> <p>In higher dimensions (where the embedding dimension \(d\) is even), RoPE generalizes the 2D rotation across all pairs of dimensions in the embedding space.</p> <p>This is achieved by dividing the \(d\)-dimensional space into \(d/2\) sub-blocks and applying a 2D rotation to each sub-block. The generalized form of RoPE is given by:</p> \[f_{\{ q, k \} }\left( \boldsymbol{x}_{m}, m \right) = \boldsymbol{R}_{\Theta, m}^{d} \boldsymbol{W}_{ \{ q, k \} } \boldsymbol{x}_m\] <p>where \(\boldsymbol{R}_{\Theta, m}^{d}\) is the block-diagonal rotation matrix for position \(m\), constructed as:</p> \[\boldsymbol{R}_{\Theta, m}^{d} = \begin{pmatrix} \boldsymbol{R}_{\theta_1, m} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \boldsymbol{R}_{\theta_2, m} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \boldsymbol{R}_{\theta_{d/2}, m} \end{pmatrix}\] <p>Each block \(\boldsymbol{R}_{\theta_i, m}\) is a 2D rotation matrix:</p> \[\boldsymbol{R}_{\theta_i, m} = \begin{pmatrix} \cos m \theta_i &amp; -\sin m \theta_i \\ \sin m \theta_i &amp; \cos m \theta_i \end{pmatrix}\] <p>with \(\theta_i = 10000^{-2(i-1)/d}\) representing the rotation angles for each sub-block, ensuring a diverse encoding of positions across different dimensions.</p> <hr/> <p>By applying these positional encoding methods, transformers can effectively incorporate sequence order information into the model, enhancing their ability to understand and generate language with respect to word positions.</p>]]></content><author><name></name></author><category term="NLP,"/><category term="Machine"/><category term="Learning"/><category term="transformers,"/><category term="positional"/><category term="encoding,"/><category term="deep"/><category term="learning"/><summary type="html"><![CDATA[A summary of positional encoding methods in transformers]]></summary></entry><entry><title type="html">Representation Engineering: A Top-Down Approach to AI Transparency</title><link href="https://longjubai.github.io/blog/2024/Representation-Engineering/" rel="alternate" type="text/html" title="Representation Engineering: A Top-Down Approach to AI Transparency"/><published>2024-07-15T00:00:00+00:00</published><updated>2024-07-15T00:00:00+00:00</updated><id>https://longjubai.github.io/blog/2024/Representation-Engineering</id><content type="html" xml:base="https://longjubai.github.io/blog/2024/Representation-Engineering/"><![CDATA[<h3 id="introduction-to-representation-engineering-repe">Introduction to Representation Engineering (RepE)</h3> <p>Representation Engineering (RepE) offers a complementary research paradigm for understanding and controlling Large Language Models (LLMs), running parallel to Mechanistic Interpretability (MI). While <strong>MI</strong> primarily operates at the level of circuits—such as attention heads, pathways, and neurons—<strong>RepE</strong> focuses on the <strong>representational spaces</strong> (the inner activations at each layer). Both approaches aim to increase the transparency and control of LLM behavior.</p> <p>Some potential applications of Representation Engineering include:</p> <ul> <li><strong>Jailbreaking</strong>: Identifying and controlling prompts that bypass model restrictions.</li> <li><strong>Motion Forecasting</strong>: Predicting movement patterns based on representation spaces.</li> </ul> <p>For more insights, refer to these resources:</p> <ul> <li><a href="https://vgel.me/posts/representation-engineering/">Representation Engineering Blog by vgel.me</a></li> <li><a href="https://www.alignmentforum.org/posts/3ghj8EuKzwD3MQR5G/an-introduction-to-representation-engineering-an-activation#Activation_Patching">An Introduction to Representation Engineering on Alignment Forum</a></li> </ul> <hr/> <h3 id="structure-of-the-repe-paper">Structure of the RepE Paper</h3> <p>The RepE paper is divided into two primary sections: <strong>Representation Reading</strong> and <strong>Representation Control</strong>.</p> <hr/> <h3 id="part-1-representation-reading">Part 1: Representation Reading</h3> <p><strong>Representation Reading</strong> aims to identify a <strong>reading vector</strong> in the activation space that aligns with a specific high-level concept, behavior, or function. This vector, also known as a <strong>concept vector</strong>, helps detect or control certain concepts within the model’s internal representation. Below are the key steps to derive this vector:</p> <ol> <li> <p><strong>Design Contrastive Inputs</strong>: Create inputs that stimulate the model’s inner activity regarding specific concepts, often using pairwise contrasts (e.g., harmful vs. harmless, honest vs. dishonest).</p> </li> <li> <p><strong>Feed Inputs and Collect Activations</strong>: Pass the designed inputs into the model and collect activations (representations) from specific layers (often from the last token’s representation).</p> </li> <li> <p><strong>Apply Dimensionality Reduction and Clustering</strong>: Use techniques like <strong>Principal Component Analysis (PCA)</strong>, <strong>K-Means</strong>, or other supervised or unsupervised methods to find a linear vector that effectively separates the two contrasting classes (e.g., harmful vs. harmless).</p> </li> </ol> <h4 id="usage-of-the-reading-vector">Usage of the Reading Vector</h4> <p>To make predictions, compute the <strong>dot product</strong> between the concept vector and a target representation vector. This indicates whether the target vector contains the concept. Alternatively, you can scan activations across all layers to assess the strength of a specific concept throughout the model.</p> <hr/> <h3 id="part-2-representation-control">Part 2: Representation Control</h3> <p><strong>Representation Control</strong> (also called <strong>Representation Steering</strong>) involves modifying the activations of the LLM during a forward pass to influence its behavior. This section explores ways to control the internal representations of concepts and functions within the model.</p> <h4 id="step-1-choose-a-controller">Step 1: Choose a Controller</h4> <p>There are several options for selecting a controller:</p> <ul> <li><strong>Reading Vector</strong>: The vector extracted from the Representation Reading process.</li> <li><strong>Contrast Vector</strong>: Derived from a pair of contrastive prompts during inference. The difference between the representations of these prompts forms a Contrast Vector.</li> <li><strong>Low-Rank Adapter</strong>: Fine-tune low-rank adapters linked to the model, applying a specific loss function to representations.</li> </ul> <h4 id="step-2-choose-an-intervention-method">Step 2: Choose an Intervention Method</h4> <p>After selecting a controller, choose an operator to apply to the model’s inner activations:</p> <ul> <li><strong>Linear Combination</strong>: Blend vectors linearly to steer activations.</li> <li><strong>Piece-Wise Operation</strong>: Apply different transformations to parts of the vector.</li> <li><strong>Projection</strong>: Project onto or away from certain concept directions.</li> <li><strong>Adapter</strong>: Use fine-tuned adapters to influence representations based on the desired concept or function.</li> </ul> <hr/> <p>This blog post provides an overview of <strong>Representation Engineering</strong> as a structured approach to understanding and guiding LLM behaviors. By working within representational spaces rather than specific neurons or pathways, RepE opens new possibilities for transparent and controllable AI.</p>]]></content><author><name></name></author><category term="research"/><category term="representation-engineering"/><category term="interpretability"/><category term="AI"/><category term="transparency"/><summary type="html"><![CDATA[Exploring Representation Engineering (RepE) as a parallel research paradigm for interpretability and controllability in LLMs.]]></summary></entry></feed>