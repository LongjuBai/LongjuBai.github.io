<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://longjubai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://longjubai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-04T03:41:06+00:00</updated><id>https://longjubai.github.io/feed.xml</id><title type="html">Longju Bai’s Homepage</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://longjubai.github.io/blog/2024/2024-07-15-Representation-Engineering/" rel="alternate" type="text/html" title=""/><published>2024-11-04T03:41:06+00:00</published><updated>2024-11-04T03:41:06+00:00</updated><id>https://longjubai.github.io/blog/2024/2024-07-15-Representation-Engineering</id><content type="html" xml:base="https://longjubai.github.io/blog/2024/2024-07-15-Representation-Engineering/"><![CDATA[<h3 id="introduction-to-representation-engineering-repe">Introduction to Representation Engineering (RepE)</h3> <p>Representation Engineering (RepE) offers a complementary research paradigm for understanding and controlling Large Language Models (LLMs), running parallel to Mechanistic Interpretability (MI). While <strong>MI</strong> primarily operates at the level of circuits—such as attention heads, pathways, and neurons—<strong>RepE</strong> focuses on the <strong>representational spaces</strong> (the inner activations at each layer). Both approaches aim to increase the transparency and control of LLM behavior.</p> <p>Some potential applications of Representation Engineering include:</p> <ul> <li><strong>Jailbreaking</strong>: Identifying and controlling prompts that bypass model restrictions.</li> <li><strong>Motion Forecasting</strong>: Predicting movement patterns based on representation spaces.</li> </ul> <p>For more insights, refer to these resources:</p> <ul> <li><a href="https://vgel.me/posts/representation-engineering/">Representation Engineering Blog by vgel.me</a></li> <li><a href="https://www.alignmentforum.org/posts/3ghj8EuKzwD3MQR5G/an-introduction-to-representation-engineering-an-activation#Activation_Patching">An Introduction to Representation Engineering on Alignment Forum</a></li> </ul> <hr/> <h3 id="structure-of-the-repe-paper">Structure of the RepE Paper</h3> <p>The RepE paper is divided into two primary sections: <strong>Representation Reading</strong> and <strong>Representation Control</strong>.</p> <hr/> <h3 id="part-1-representation-reading">Part 1: Representation Reading</h3> <p><strong>Representation Reading</strong> aims to identify a <strong>reading vector</strong> in the activation space that aligns with a specific high-level concept, behavior, or function. This vector, also known as a <strong>concept vector</strong>, helps detect or control certain concepts within the model’s internal representation. Below are the key steps to derive this vector:</p> <ol> <li> <p><strong>Design Contrastive Inputs</strong>: Create inputs that stimulate the model’s inner activity regarding specific concepts, often using pairwise contrasts (e.g., harmful vs. harmless, honest vs. dishonest).</p> </li> <li> <p><strong>Feed Inputs and Collect Activations</strong>: Pass the designed inputs into the model and collect activations (representations) from specific layers (often from the last token’s representation).</p> </li> <li> <p><strong>Apply Dimensionality Reduction and Clustering</strong>: Use techniques like <strong>Principal Component Analysis (PCA)</strong>, <strong>K-Means</strong>, or other supervised or unsupervised methods to find a linear vector that effectively separates the two contrasting classes (e.g., harmful vs. harmless).</p> </li> </ol> <h4 id="usage-of-the-reading-vector">Usage of the Reading Vector</h4> <p>To make predictions, compute the <strong>dot product</strong> between the concept vector and a target representation vector. This indicates whether the target vector contains the concept. Alternatively, you can scan activations across all layers to assess the strength of a specific concept throughout the model.</p> <hr/> <h3 id="part-2-representation-control">Part 2: Representation Control</h3> <p><strong>Representation Control</strong> (also called <strong>Representation Steering</strong>) involves modifying the activations of the LLM during a forward pass to influence its behavior. This section explores ways to control the internal representations of concepts and functions within the model.</p> <h4 id="step-1-choose-a-controller">Step 1: Choose a Controller</h4> <p>There are several options for selecting a controller:</p> <ul> <li><strong>Reading Vector</strong>: The vector extracted from the Representation Reading process.</li> <li><strong>Contrast Vector</strong>: Derived from a pair of contrastive prompts during inference. The difference between the representations of these prompts forms a Contrast Vector.</li> <li><strong>Low-Rank Adapter</strong>: Fine-tune low-rank adapters linked to the model, applying a specific loss function to representations.</li> </ul> <h4 id="step-2-choose-an-intervention-method">Step 2: Choose an Intervention Method</h4> <p>After selecting a controller, choose an operator to apply to the model’s inner activations:</p> <ul> <li><strong>Linear Combination</strong>: Blend vectors linearly to steer activations.</li> <li><strong>Piece-Wise Operation</strong>: Apply different transformations to parts of the vector.</li> <li><strong>Projection</strong>: Project onto or away from certain concept directions.</li> <li><strong>Adapter</strong>: Use fine-tuned adapters to influence representations based on the desired concept or function.</li> </ul> <hr/> <p>This blog post provides an overview of <strong>Representation Engineering</strong> as a structured approach to understanding and guiding LLM behaviors. By working within representational spaces rather than specific neurons or pathways, RepE opens new possibilities for transparent and controllable AI.</p>]]></content><author><name></name></author></entry><entry><title type="html">A Brief Summary on Positional Encoding Methods</title><link href="https://longjubai.github.io/blog/2024/Positional-Encoding/" rel="alternate" type="text/html" title="A Brief Summary on Positional Encoding Methods"/><published>2024-08-01T00:00:00+00:00</published><updated>2024-08-01T00:00:00+00:00</updated><id>https://longjubai.github.io/blog/2024/Positional-Encoding</id><content type="html" xml:base="https://longjubai.github.io/blog/2024/Positional-Encoding/"><![CDATA[<h3 id="preliminary">Preliminary</h3> <p>Unlike recurrent architectures, transformers process inputs in parallel all at once. However, self-attention does not consider the position of a word in sequence, making sentences like “I think therefore I am” and “I am therefore I think” appear the same.</p> <p>The notations used are as follows:</p> <ul> <li>( S_N = { w_i }_{i=1}^N ): a sequence of ( N ) input tokens with ( w_i ) being the ( i )-th element.</li> <li>( E_N = { x_i }_{i=1}^N ): corresponding word embeddings of ( S_N ), where ( x_i \in \mathbb{R}^d ) is the ( d )-dimensional word embedding vector of token ( w_i ) without positional information.</li> </ul> <p>The positional information is incorporated into ( q_m ), ( k_n ), and ( v_n ) for the ( m )-th and ( n )-th positions through functions ( f_q ), ( f_k ), and ( f_v ), respectively:</p> <p>[ q_m = f_q(x_m, m), \quad k_n = f_k(x_n, n), \quad v_n = f_v(x_n, n) ]</p> <p>The query and key values are then used to compute the attention weights, and the output is computed as the weighted sum over the value representation:</p> <p>[ a_{m,n} = \frac{\exp\left(\frac{q_m^T k_n}{\sqrt{d}}\right)}{\sum_{j=1}^{N} \exp\left(\frac{q_m^T k_j}{\sqrt{d}}\right)}, \quad o_m = \sum_{n=1}^{N} a_{m,n} v_n ]</p> <p>Existing transformer-based positional encoding methods primarily focus on selecting a suitable function to form the equation above.</p> <hr/> <h3 id="absolute-positional-encoding-ape">Absolute Positional Encoding (APE)</h3> <p>There are typically two forms of Absolute Positional Encoding (APE):</p> <ol> <li> <p><strong>Trainable Position Vectors</strong> ( \boldsymbol{p}<em>{i} \in\left{\boldsymbol{p}</em>{t}\right}<em>{t=1}^{L} ): [ f</em>{t: t \in{q, k, v}}\left(\boldsymbol{x}<em>{i}, i\right):=\boldsymbol{W}</em>{t: t \in{q, k, v}}\left(\boldsymbol{x}<em>{i}+\boldsymbol{p}</em>{i}\right) ] where ( L ) is the maximum sequence length. This form is used in the GPT-2 model, which means there won’t be a position embedding for unseen input lengths after the maximum length during training.</p> </li> <li> <p><strong>Sinusoidal Positional Encoding</strong>: [ \begin{cases} \boldsymbol{p}<em>{i, 2 t} &amp; =\sin \left(k / 10000^{2 t / d}\right) <br/> \boldsymbol{p}</em>{i, 2 t+1} &amp; =\cos \left(k / 10000^{2 t / d}\right) \end{cases} ]</p> </li> </ol> <hr/> <h3 id="relative-positional-encoding-repe">Relative Positional Encoding (RePE)</h3> <p>Relative Positional Encoding (RePE) was first proposed by Shaw et al. in 2018, and a simplified version is used in the T5 model:</p> <ul> <li> <p><strong>Shaw’s Version</strong>: [ \begin{array}{r} f_{q}\left(\boldsymbol{x}<em>{m}\right):=\boldsymbol{W}</em>{q} \boldsymbol{x}<em>{m} <br/> f</em>{k}\left(\boldsymbol{x}<em>{n}, n\right):=\boldsymbol{W}</em>{k}\left(\boldsymbol{x}<em>{n}+\tilde{\boldsymbol{p}}</em>{r}^{k}\right)<br/> f_{v}\left(\boldsymbol{x}<em>{n}, n\right):=\boldsymbol{W}</em>{v}\left(\boldsymbol{x}<em>{n}+\tilde{\boldsymbol{p}}</em>{r}^{v}\right) \end{array} ] where ( \tilde{\boldsymbol{p}}<em>{r}^{k}, \tilde{\boldsymbol{p}}</em>{r}^{v} \in \mathbb{R}^{d} ) are trainable relative position embeddings, with ( r = \operatorname{clip}\left(m-n, r_{\min }, r_{\max }\right) ) representing the relative distance between positions ( m ) and ( n ).</p> </li> <li> <p><strong>T5 Simplified Version</strong>: The relative position encoding is applied directly as a scalar bias to the logits of the attention mechanism. The attention logit for token ( m ) attending to token ( n ) is computed as: [ \text{Logit}(m, n) = \frac{\boldsymbol{q}_m \cdot \boldsymbol{k}_n}{\sqrt{d}} + b_r ]</p> </li> </ul> <hr/> <h3 id="rotary-positional-encoding-rope">Rotary Positional Encoding (RoPE)</h3> <p>Instead of adding positional embedding vectors, RoPE applies a rotation to the representation vectors before computing their dot products, integrating both absolute and relative positional information.</p> <h4 id="2d-form">2D Form</h4> <p>In a 2-dimensional space, RoPE applies the following transformation:</p> <p>[ f_{{q, k}}\left(\boldsymbol{x}<em>{m}, m\right) = \left(\begin{array}{cc} \cos m \theta &amp; -\sin m \theta <br/> \sin m \theta &amp; \cos m \theta \end{array}\right) \boldsymbol{W}</em>{{q, k}} \boldsymbol{x}_m ]</p> <p>where:</p> <ul> <li>( \boldsymbol{W}_{{q, k}} ) are the weight matrices for queries and keys.</li> <li>( m ) is the current position index.</li> <li>( \theta ) is a constant that determines the rate of rotation per position step, encoding the absolute position.</li> </ul> <p>The attention mechanism then calculates the dot product between these rotated embeddings.</p> <h4 id="general-form">General Form</h4> <p>For higher dimensions (where the embedding dimension ( d ) is even), RoPE generalizes this 2D rotation across all pairs of dimensions in the embedding space by dividing it into ( d/2 ) sub-blocks. Each sub-block undergoes a 2D rotation. The generalized form of RoPE is:</p> <p>[ f_{{q, k}}\left(\boldsymbol{x}<em>{m}, m\right) = \boldsymbol{R}</em>{\Theta, m}^{d} \boldsymbol{W}_{{q, k}} \boldsymbol{x}_m ]</p> <p>where ( \boldsymbol{R}_{\Theta, m}^{d} ) is the block-diagonal rotation matrix for position ( m ), constructed as:</p> <p>[ \boldsymbol{R}<em>{\Theta, m}^{d} = \left(\begin{array}{cccc} \boldsymbol{R}</em>{\theta_1, m} &amp; 0 &amp; \cdots &amp; 0 <br/> 0 &amp; \boldsymbol{R}<em>{\theta_2, m} &amp; \cdots &amp; 0 <br/> \vdots &amp; \vdots &amp; \ddots &amp; \vdots <br/> 0 &amp; 0 &amp; \cdots &amp; \boldsymbol{R}</em>{\theta_{d/2}, m} \end{array}\right) ]</p> <p>Each block ( \boldsymbol{R}_{\theta_i, m} ) is a 2D rotation matrix:</p> <p>[ \boldsymbol{R}_{\theta_i, m} = \left(\begin{array}{cc} \cos m \theta_i &amp; -\sin m \theta_i <br/> \sin m \theta_i &amp; \cos m \theta_i \end{array}\right) ]</p> <p>with ( \theta_i = 10000^{-2(i-1)/d} ) representing the rotation angles for each sub-block, encoding positions across different dimensions.</p> <hr/> <p>This blog post provides a high-level overview of the primary positional encoding methods in transformers, including Absolute Positional Encoding, Relative Positional Encoding, and Rotary Positional Encoding, each with unique mechanisms for encoding positional information in transformer architectures.</p>]]></content><author><name></name></author><category term="research"/><category term="transformers"/><category term="positional-encoding"/><category term="machine-learning"/><summary type="html"><![CDATA[An overview of different positional encoding techniques used in transformers.]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://longjubai.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://longjubai.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://longjubai.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://longjubai.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://longjubai.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://longjubai.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>