<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Brief Summary on Positional Encoding Methods | Longju Bai's Homepage </title> <meta name="author" content="Longju Bai"> <meta name="description" content="A summary of positional encoding methods in transformers"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://longjubai.github.io/blog/2024/Positional-Encoding/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Longju Bai's Homepage </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_LongjuBai.pdf">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Brief Summary on Positional Encoding Methods</h1> <p class="post-meta"> Created in August 22, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>   <a href="/blog/tag/positional-encoding"> <i class="fa-solid fa-hashtag fa-sm"></i> positional-encoding</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> Research</a>   <a href="/blog/category/nlp-foundations"> <i class="fa-solid fa-tag fa-sm"></i> NLP-Foundations</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="preliminary">Preliminary</h2> <p>Unlike recurrent architectures, transformers process inputs in parallel all at once. However, self-attention doesn’t consider the position of a word in a sequence; thus, sentences like “I think therefore I am” and “I am therefore I think” are treated the same.</p> <p>The notations are shown below:</p> <ul> <li>\(S_N = \{ w_i \}_{i=1}^N\): a sequence of \(N\) input tokens with \(w_i\) being the \(i\)-th element.</li> <li>\(E_N = \{ x_i \}_{i=1}^N\): corresponding word embeddings of \(S_N\); \(x_i \in \mathbb{R}^d\) is the \(d\)-dimensional word embedding vector of token \(w_i\) without position information.</li> </ul> <p>\(q_m\), \(k_n\), and \(v_n\) incorporate the \(m\)-th and \(n\)-th positions through \(f_q\), \(f_k\), and \(f_v\), respectively.</p> \[q_m = f_q(x_m, m), \quad k_n = f_k(x_n, n), \quad v_n = f_v(x_n, n) \tag{1}\] <p>The query and key values are then used to compute the attention weights, while the output is computed as the weighted sum over the value representations.</p> \[a_{m,n} = \frac{\exp\left( \frac{ q_m^\top k_n }{ \sqrt{d} } \right)}{ \sum_{j=1}^{N} \exp\left( \frac{ q_m^\top k_j }{ \sqrt{d} } \right)}, \quad o_m = \sum_{n=1}^{N} a_{m,n} v_n \tag{2}\] <p>The existing approaches of transformer-based positional encoding mainly focus on choosing a suitable function to form Equation (1).</p> <hr> <h2 id="absolute-positional-encoding-ape">Absolute Positional Encoding (APE)</h2> <p>There are usually two forms of APE:</p> <ul> <li> <p><strong>Trainable position vectors</strong> \(\boldsymbol{p}_{i} \in \{ \boldsymbol{p}_{t} \}_{t=1}^{L}\), where \(L\) is the maximum sequence length.</p> \[f_{t \in \{ q, k, v \} }\left( \boldsymbol{x}_{i}, i \right) := \boldsymbol{W}_{t \in \{ q, k, v \} } \left( \boldsymbol{x}_{i} + \boldsymbol{p}_{i} \right) \tag{3}\] </li> <li> <p><strong>Generate \(\boldsymbol{p}_{i}\) using the sinusoidal function:</strong></p> \[\begin{cases} \boldsymbol{p}_{i, 2t} = \sin \left( \frac{i}{10000^{2t / d}} \right), \\ \boldsymbol{p}_{i, 2t+1} = \cos \left( \frac{i}{10000^{2t / d}} \right) \end{cases} \tag{4}\] </li> </ul> <p>The first form is used in the GPT-2 model; thus, when facing unseen input lengths, there will be no position embeddings beyond the maximum length encountered during training.</p> <hr> <h2 id="relative-positional-encoding-repe">Relative Positional Encoding (RePE)</h2> <p>We introduce the original RePE proposed by Shaw et al. in 2018 and a simplified version used in the T5 model:</p> <ul> <li> <p><strong>Shaw’s Version:</strong></p> \[\begin{aligned} f_{q}\left( \boldsymbol{x}_{m} \right) &amp;= \boldsymbol{W}_{q} \boldsymbol{x}_{m}, \\ f_{k}\left( \boldsymbol{x}_{n}, n \right) &amp;= \boldsymbol{W}_{k} \left( \boldsymbol{x}_{n} + \tilde{\boldsymbol{p}}_{r}^{k} \right), \\ f_{v}\left( \boldsymbol{x}_{n}, n \right) &amp;= \boldsymbol{W}_{v} \left( \boldsymbol{x}_{n} + \tilde{\boldsymbol{p}}_{r}^{v} \right) \end{aligned} \tag{5}\] <p>where \(\tilde{\boldsymbol{p}}_{r}^{k}, \tilde{\boldsymbol{p}}_{r}^{v} \in \mathbb{R}^{d}\) are trainable relative position embeddings. Note that \(r = \operatorname{clip}\left( m - n, r_{\min}, r_{\max} \right)\) represents the relative distance between positions \(m\) and \(n\).</p> <p>For example, if we set the maximum relative length to be 4, then a sequence of 5 words will have a total of 9 embeddings to be learned (1 embedding for the current word, 4 embeddings for the words to the left, and 4 embeddings for the words to the right of the current word).</p> </li> <li> <p><strong>Simplified Version Used in T5 Model:</strong></p> <p>The relative position encoding is applied directly as a scalar bias to the logits of the attention mechanism.</p> <p>Specifically, the scalar bias for each relative position \(r = m - n\) is added to the dot product of the query and key vectors before the softmax operation.</p> <p>The attention logit for token \(m\) attending to token \(n\) is computed as:</p> \[\text{Logit}(m, n) = \frac{ \boldsymbol{q}_m \cdot \boldsymbol{k}_n }{ \sqrt{d} } + b_r\] </li> </ul> <hr> <h2 id="rotary-positional-encoding-rope">Rotary Positional Encoding (RoPE)</h2> <p>Instead of simply adding positional embedding vectors, RoPE applies a rotation to the representation vectors before computing their dot products, integrating both absolute and relative position information.</p> <h3 id="2d-form">2D Form</h3> <p>For a 2-dimensional space, the transformation can be described as:</p> \[f_{\{ q, k \} }\left( \boldsymbol{x}_{m}, m \right) = \begin{pmatrix} \cos m \theta &amp; -\sin m \theta \\ \sin m \theta &amp; \cos m \theta \end{pmatrix} \boldsymbol{W}_{ \{ q, k \} } \boldsymbol{x}_m \tag{6}\] <p>where:</p> <ul> <li>\(\boldsymbol{W}_{\{ q, k \} }\) are the weight matrices for queries and keys.</li> <li>\(m\) is the current position index.</li> <li>\(\theta\) is a constant that determines the rate of rotation per position step, effectively encoding the absolute position.</li> </ul> <p>The attention mechanism then calculates the dot product between these rotated embeddings.</p> <h3 id="general-form">General Form</h3> <p>In higher dimensions (where the embedding dimension \(d\) is even), RoPE generalizes the 2D rotation across all pairs of dimensions in the embedding space.</p> <p>This is achieved by dividing the \(d\)-dimensional space into \(d/2\) sub-blocks and applying a 2D rotation to each sub-block. The generalized form of RoPE is given by:</p> \[f_{\{ q, k \} }\left( \boldsymbol{x}_{m}, m \right) = \boldsymbol{R}_{\Theta, m}^{d} \boldsymbol{W}_{ \{ q, k \} } \boldsymbol{x}_m \tag{7}\] <p>where \(\boldsymbol{R}_{\Theta, m}^{d}\) is the block-diagonal rotation matrix for position \(m\), constructed as:</p> \[\boldsymbol{R}_{\Theta, m}^{d} = \begin{pmatrix} \boldsymbol{R}_{\theta_1, m} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \boldsymbol{R}_{\theta_2, m} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \boldsymbol{R}_{\theta_{d/2}, m} \end{pmatrix}\] <p>Each block \(\boldsymbol{R}_{\theta_i, m}\) is a 2D rotation matrix:</p> \[\boldsymbol{R}_{\theta_i, m} = \begin{pmatrix} \cos m \theta_i &amp; -\sin m \theta_i \\ \sin m \theta_i &amp; \cos m \theta_i \end{pmatrix}\] <p>with \(\theta_i = 10000^{-2(i-1)/d}\) representing the rotation angles for each sub-block, ensuring a diverse encoding of positions across different dimensions.</p> <hr> <p>By applying these positional encoding methods, transformers can effectively incorporate sequence order information into the model, enhancing their ability to understand and generate language with respect to word positions.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Representation-Engineering/">Representation Engineering: A Top-Down Approach to AI Transparency</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Longju Bai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 04, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-teaching",title:"teaching",description:"Materials for courses you taught. Replace this text with your description.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/assets/pdf/CV_LongjuBai.pdf"}},{id:"post-a-brief-summary-on-positional-encoding-methods",title:"A Brief Summary on Positional Encoding Methods",description:"A summary of positional encoding methods in transformers",section:"Posts",handler:()=>{window.location.href="/blog/2024/Positional-Encoding/"}},{id:"post-representation-engineering-a-top-down-approach-to-ai-transparency",title:"Representation Engineering: A Top-Down Approach to AI Transparency",description:"Exploring Representation Engineering (RepE) as a parallel research paradigm for interpretability and controllability in LLMs.",section:"Posts",handler:()=>{window.location.href="/blog/2024/Representation-Engineering/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%6F%6E%67%6A%75@%75%6D%69%63%68.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>