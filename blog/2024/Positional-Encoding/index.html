<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Brief Summary on Positional Encoding Methods | Longju Bai's Homepage </title> <meta name="author" content="Longju Bai"> <meta name="description" content="An overview of different positional encoding techniques used in transformers."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://longjubai.github.io/blog/2024/Positional-Encoding/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Longju Bai's Homepage </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV_LongjuBai.pdf">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Brief Summary on Positional Encoding Methods</h1> <p class="post-meta"> Created in August 01, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>   <a href="/blog/tag/positional-encoding"> <i class="fa-solid fa-hashtag fa-sm"></i> positional-encoding</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="preliminary">Preliminary</h3> <p>Unlike recurrent architectures, transformers process inputs in parallel all at once. However, self-attention does not consider the position of a word in sequence, making sentences like “I think therefore I am” and “I am therefore I think” appear the same.</p> <p>The notations used are as follows:</p> <ul> <li>( S_N = { w_i }_{i=1}^N ): a sequence of ( N ) input tokens with ( w_i ) being the ( i )-th element.</li> <li>( E_N = { x_i }_{i=1}^N ): corresponding word embeddings of ( S_N ), where ( x_i \in \mathbb{R}^d ) is the ( d )-dimensional word embedding vector of token ( w_i ) without positional information.</li> </ul> <p>The positional information is incorporated into ( q_m ), ( k_n ), and ( v_n ) for the ( m )-th and ( n )-th positions through functions ( f_q ), ( f_k ), and ( f_v ), respectively:</p> <p>[ q_m = f_q(x_m, m), \quad k_n = f_k(x_n, n), \quad v_n = f_v(x_n, n) ]</p> <p>The query and key values are then used to compute the attention weights, and the output is computed as the weighted sum over the value representation:</p> <p>[ a_{m,n} = \frac{\exp\left(\frac{q_m^T k_n}{\sqrt{d}}\right)}{\sum_{j=1}^{N} \exp\left(\frac{q_m^T k_j}{\sqrt{d}}\right)}, \quad o_m = \sum_{n=1}^{N} a_{m,n} v_n ]</p> <p>Existing transformer-based positional encoding methods primarily focus on selecting a suitable function to form the equation above.</p> <hr> <h3 id="absolute-positional-encoding-ape">Absolute Positional Encoding (APE)</h3> <p>There are typically two forms of Absolute Positional Encoding (APE):</p> <ol> <li> <p><strong>Trainable Position Vectors</strong> ( \boldsymbol{p}<em>{i} \in\left{\boldsymbol{p}</em>{t}\right}<em>{t=1}^{L} ): [ f</em>{t: t \in{q, k, v}}\left(\boldsymbol{x}<em>{i}, i\right):=\boldsymbol{W}</em>{t: t \in{q, k, v}}\left(\boldsymbol{x}<em>{i}+\boldsymbol{p}</em>{i}\right) ] where ( L ) is the maximum sequence length. This form is used in the GPT-2 model, which means there won’t be a position embedding for unseen input lengths after the maximum length during training.</p> </li> <li> <p><strong>Sinusoidal Positional Encoding</strong>: [ \begin{cases} \boldsymbol{p}<em>{i, 2 t} &amp; =\sin \left(k / 10000^{2 t / d}\right) <br> \boldsymbol{p}</em>{i, 2 t+1} &amp; =\cos \left(k / 10000^{2 t / d}\right) \end{cases} ]</p> </li> </ol> <hr> <h3 id="relative-positional-encoding-repe">Relative Positional Encoding (RePE)</h3> <p>Relative Positional Encoding (RePE) was first proposed by Shaw et al. in 2018, and a simplified version is used in the T5 model:</p> <ul> <li> <p><strong>Shaw’s Version</strong>: [ \begin{array}{r} f_{q}\left(\boldsymbol{x}<em>{m}\right):=\boldsymbol{W}</em>{q} \boldsymbol{x}<em>{m} <br> f</em>{k}\left(\boldsymbol{x}<em>{n}, n\right):=\boldsymbol{W}</em>{k}\left(\boldsymbol{x}<em>{n}+\tilde{\boldsymbol{p}}</em>{r}^{k}\right)<br> f_{v}\left(\boldsymbol{x}<em>{n}, n\right):=\boldsymbol{W}</em>{v}\left(\boldsymbol{x}<em>{n}+\tilde{\boldsymbol{p}}</em>{r}^{v}\right) \end{array} ] where ( \tilde{\boldsymbol{p}}<em>{r}^{k}, \tilde{\boldsymbol{p}}</em>{r}^{v} \in \mathbb{R}^{d} ) are trainable relative position embeddings, with ( r = \operatorname{clip}\left(m-n, r_{\min }, r_{\max }\right) ) representing the relative distance between positions ( m ) and ( n ).</p> </li> <li> <p><strong>T5 Simplified Version</strong>: The relative position encoding is applied directly as a scalar bias to the logits of the attention mechanism. The attention logit for token ( m ) attending to token ( n ) is computed as: [ \text{Logit}(m, n) = \frac{\boldsymbol{q}_m \cdot \boldsymbol{k}_n}{\sqrt{d}} + b_r ]</p> </li> </ul> <hr> <h3 id="rotary-positional-encoding-rope">Rotary Positional Encoding (RoPE)</h3> <p>Instead of adding positional embedding vectors, RoPE applies a rotation to the representation vectors before computing their dot products, integrating both absolute and relative positional information.</p> <h4 id="2d-form">2D Form</h4> <p>In a 2-dimensional space, RoPE applies the following transformation:</p> <p>[ f_{{q, k}}\left(\boldsymbol{x}<em>{m}, m\right) = \left(\begin{array}{cc} \cos m \theta &amp; -\sin m \theta <br> \sin m \theta &amp; \cos m \theta \end{array}\right) \boldsymbol{W}</em>{{q, k}} \boldsymbol{x}_m ]</p> <p>where:</p> <ul> <li>( \boldsymbol{W}_{{q, k}} ) are the weight matrices for queries and keys.</li> <li>( m ) is the current position index.</li> <li>( \theta ) is a constant that determines the rate of rotation per position step, encoding the absolute position.</li> </ul> <p>The attention mechanism then calculates the dot product between these rotated embeddings.</p> <h4 id="general-form">General Form</h4> <p>For higher dimensions (where the embedding dimension ( d ) is even), RoPE generalizes this 2D rotation across all pairs of dimensions in the embedding space by dividing it into ( d/2 ) sub-blocks. Each sub-block undergoes a 2D rotation. The generalized form of RoPE is:</p> <p>[ f_{{q, k}}\left(\boldsymbol{x}<em>{m}, m\right) = \boldsymbol{R}</em>{\Theta, m}^{d} \boldsymbol{W}_{{q, k}} \boldsymbol{x}_m ]</p> <p>where ( \boldsymbol{R}_{\Theta, m}^{d} ) is the block-diagonal rotation matrix for position ( m ), constructed as:</p> <p>[ \boldsymbol{R}<em>{\Theta, m}^{d} = \left(\begin{array}{cccc} \boldsymbol{R}</em>{\theta_1, m} &amp; 0 &amp; \cdots &amp; 0 <br> 0 &amp; \boldsymbol{R}<em>{\theta_2, m} &amp; \cdots &amp; 0 <br> \vdots &amp; \vdots &amp; \ddots &amp; \vdots <br> 0 &amp; 0 &amp; \cdots &amp; \boldsymbol{R}</em>{\theta_{d/2}, m} \end{array}\right) ]</p> <p>Each block ( \boldsymbol{R}_{\theta_i, m} ) is a 2D rotation matrix:</p> <p>[ \boldsymbol{R}_{\theta_i, m} = \left(\begin{array}{cc} \cos m \theta_i &amp; -\sin m \theta_i <br> \sin m \theta_i &amp; \cos m \theta_i \end{array}\right) ]</p> <p>with ( \theta_i = 10000^{-2(i-1)/d} ) representing the rotation angles for each sub-block, encoding positions across different dimensions.</p> <hr> <p>This blog post provides a high-level overview of the primary positional encoding methods in transformers, including Absolute Positional Encoding, Relative Positional Encoding, and Rotary Positional Encoding, each with unique mechanisms for encoding positional information in transformer architectures.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Representation-Engineering/">Representation Engineering: A Top-Down Approach to AI Transparency</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Longju Bai. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: November 04, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-teaching",title:"teaching",description:"Materials for courses you taught. Replace this text with your description.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/assets/pdf/CV_LongjuBai.pdf"}},{id:"post-a-brief-summary-on-positional-encoding-methods",title:"A Brief Summary on Positional Encoding Methods",description:"An overview of different positional encoding techniques used in transformers.",section:"Posts",handler:()=>{window.location.href="/blog/2024/Positional-Encoding/"}},{id:"post-representation-engineering-a-top-down-approach-to-ai-transparency",title:"Representation Engineering: A Top-Down Approach to AI Transparency",description:"Exploring Representation Engineering (RepE) as a parallel research paradigm for interpretability and controllability in LLMs.",section:"Posts",handler:()=>{window.location.href="/blog/2024/Representation-Engineering/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%6F%6E%67%6A%75@%75%6D%69%63%68.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>